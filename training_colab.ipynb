{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9JJ6lslHHnJ",
        "outputId": "ef8cf4f0-85cd-4992-f572-16417834927f"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Clone the B-LoRA repository\n",
        "!git clone https://github.com/yardenfren1996/B-LoRA.git\n",
        "\n",
        "# Change directory to the cloned repository.\n",
        "%cd B-LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwbz8KrMHhm4",
        "outputId": "bf17309b-d9ab-4ecc-ecea-fd4aaf5b25ff"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install bitsandbytes --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCUfd2pANA0z",
        "outputId": "c0581028-e642-4faa-b0a2-96846e37fdc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jul 19 03:23:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   45C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accelerate Configuration Guide\n",
        "\n",
        "Follow these steps to configure `accelerate`:\n",
        "\n",
        "1. **Compute Environment:**\n",
        "   - Select: `This machine`\n",
        "\n",
        "2. **Machine Type:**\n",
        "   - Select: `No distributed training`\n",
        "\n",
        "3. **CPU or GPU:**\n",
        "   - Select: `NO` (run on GPU if available)\n",
        "\n",
        "4. **Torch Dynamo Optimization:**\n",
        "   - Select: `NO`\n",
        "\n",
        "5. **Use DeepSpeed:**\n",
        "   - Select: `NO`\n",
        "\n",
        "6. **GPU Selection:**\n",
        "   - Enter: `all` (use all available GPUs)\n",
        "\n",
        "7. **NUMA Efficiency:**\n",
        "   - Select: `NO`\n",
        "\n",
        "8. **Mixed Precision:**\n",
        "   - Select: `fp16`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36lY_8uRLvcK",
        "outputId": "38a44077-a545-4eae-8a5a-fb8c5b1121b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r----------------------------------------------------------------------------------------------------In which compute environment are you running?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            " ➔  \u001b[32mThis machine\u001b[0m\r\n",
            "    AWS (Amazon SageMaker)\n",
            "\u001b[2A\u001b[?25l0\n",
            "\u001b[32mThis machine\u001b[0m\n",
            "----------------------------------------------------------------------------------------------------Which type of machine are you using?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            " ➔  \u001b[32mNo distributed training\u001b[0m\n",
            "    multi-CPU\n",
            "    multi-XPU\n",
            "    multi-GPU\n",
            "    multi-NPU\n",
            "    multi-MLU\n",
            "    TPU\n",
            "\u001b[7A\u001b[?25l0\n",
            "\u001b[32mNo distributed training\u001b[0m\n",
            "\u001b[?25hDo you want to run your training on CPU only (even if a GPU / Apple Silicon / Ascend NPU device is available)? [yes/NO]:NO\n",
            "Do you wish to optimize your script with torch dynamo?[yes/NO]:NO\n",
            "Do you want to use DeepSpeed? [yes/NO]: NO\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:all\n",
            "Would you like to enable numa efficiency? (Currently only supported on NVIDIA hardware). [yes/NO]: NO\n",
            "----------------------------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?\n",
            "Please input a choice index (starting from 0), and press enter\n",
            " ➔  \u001b[32mno\u001b[0m\n",
            "    fp16\n",
            "    bf16\n",
            "    fp8\n",
            "\u001b[4A\u001b[?25l1\n",
            "\u001b[32mfp16\u001b[0m\n",
            "\u001b[?25haccelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Execution Guide\n",
        "\n",
        "Ensure to update the following placeholders with your specific paths and prompts:\n",
        "\n",
        "```bash\n",
        "--instance_data_dir=\"/path/to/content/\" \\\n",
        "--output_dir=\"/path/to/style\" \\\n",
        "--instance_prompt=\"DON'T FORGET TO ADD YOUR PROMPT, LOOK AT THE CELL ABOVE TO GET AN IDEA\" \\\n",
        "```\n",
        "\n",
        "### Placeholders to Change:\n",
        "\n",
        "- **`/path/to/input_dir/`**: Replace with the path to your instance data directory.\n",
        "- **`/path/to/output_dir/`**: Replace with the desired output directory path.\n",
        "- **`DON'T FORGET TO ADD YOUR PROMPT, LOOK AT THE CELL ABOVE TO GET AN IDEA`**: Replace with your specific instance prompt.\n",
        "\n",
        "### Prompt Example\n",
        "**'A [mna with long legs]'**: This is something i would use. <br>\n",
        "**'A [v54]'**: This an example from original creators, I don't like using numbers though.\n",
        "\n",
        "I've noticed that keeping the tags unqiue does help a lot, so make it meaningful and unqiue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sw26ufXHkxg",
        "outputId": "3175765b-429f-427b-91a8-dd0689ac28ec"
      },
      "outputs": [],
      "source": [
        "!accelerate launch train_dreambooth_b-lora_sdxl.py \\\n",
        " --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        " --instance_data_dir=\"/path/to/input_dir/\" \\\n",
        " --output_dir=\"/path/to/output_dir/\" \\\n",
        " --instance_prompt=\"DON'T FORGET TO ADD YOUR PROMPT, LOOK AT THE CELL ABOVE TO GET AN IDEA\" \\\n",
        " --resolution=1024 \\\n",
        " --rank=64 \\\n",
        " --train_batch_size=1 \\\n",
        " --learning_rate=5e-5 \\\n",
        " --lr_scheduler=\"constant\" \\\n",
        " --lr_warmup_steps=0 \\\n",
        " --max_train_steps=1000 \\\n",
        " --checkpointing_steps=200 \\\n",
        " --seed=\"0\" \\\n",
        " --gradient_checkpointing \\\n",
        " --use_8bit_adam \\\n",
        " --mixed_precision=\"fp16\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
